import seaborn as sns
import matplotlib.pyplot as plt
import json
import numpy as np
from enum import Enum
import pandas as pd
from scipy.optimize import curve_fit
from scipy.interpolate import make_interp_spline
from scipy.interpolate import interp1d
from numpy.polynomial.polynomial import Polynomial

# ============================================
# File: benchmark_plot.py
# Author: Lenny Del Zio
# Date: 2025-05-18
# Description: Set of functions to do plots.
# Disclaimer : Most of this code has been generated by AI.
# ============================================

def plot_mirror(data, output_dir ="output", cs = 0):
    data['mirror_size'] = data['mirror_size'].astype(str)
    plt.figure(figsize=(8, 6))
    sns.lineplot(data=data, x='error_rate', y='failure_rate', hue='mirror_size', marker="o")
    plt.xlabel("Error Rate")
    plt.ylabel("Failure Rate")
    plt.grid(True)
    plt.title("Replicas size impact on failure rate")
    plt.legend(title="Replicas factor m")
    plt.savefig(f"{output_dir}/bmirror.png")
    plt.close()

def plot_mirror2():
    data =  pd.read_pickle("test_outputs/a804eed9mirror2/test_data")
    print(data['mirror_size'].unique())
    data['mirror_size'] = data['mirror_size'].astype(str)
    plt.figure(figsize=(8, 6))
    sns.lineplot(data=data, x='error_rate', y='failure_rate', hue='mirror_size', marker="o")
    plt.xlabel("Error Rate")
    plt.ylabel("Failure Rate")
    plt.grid(True)
    plt.title("Replicas size impact on failure rate")
    plt.legend(title="Replicas factor m")
    plt.savefig(f"report_plots/bmirror20.png")
    plt.close()
    
def plot_implementation_benchmark(result, output_dir):
    plt.figure(figsize=(10, 5))

    for impl in result['implementation'].unique():
        subset = result[result['implementation'] == impl].sort_values('code_distance')
        x = subset['code_distance'].values
        y = 1 - subset['success_rate'].values
        plt.plot(x, y, label=impl)
        plt.scatter(x, y, marker='o')  # original points

    plt.title('Failure Rate vs Code Distance')
    plt.xlabel('Code Distance')
    plt.ylabel('Failure Rate')
    plt.legend()
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_dir + "/success.png")

    
    plt.figure(figsize=(10, 5))

    for impl in result['implementation'].unique():
        subset = result[result['implementation'] == impl]
        plt.plot(subset['code_distance'], subset['avg_time'], marker='o', label=impl)

    plt.title('Average Execution Time vs Code Size')
    plt.xlabel('Code Size')
    plt.ylabel('Average Time (seconds)')
    plt.yscale('log')  # optional: use log scale for better visibility
    plt.grid(True)
    plt.legend()
    plt.savefig(output_dir + "/time.png")
    

def plot_for_retrytest():
    r1 = pd.read_pickle("test_outputs/50_retry_1/test_data")
    
    r1['lfc_total_rate'] = r1['lfcx_rate'] + r1['lfcz_rate']
    
    r2 = pd.read_pickle("test_outputs/50_retry_4/test_data")
    # Merge on 'error rate'
    merged = pd.merge(r1, r2, on='error_rate', suffixes=('_No_retry', '_w_retry'))

    # Plot manually
    plt.figure(figsize=(8, 5))
    # Create the main plot
    fig, ax1 = plt.subplots(figsize=(8, 5))
    ax1.set_yscale('symlog', linthresh=1e-3)
    epsilon = 1e-6 # add a small offset so that zero value appears on the graph
    plt.grid(True)
    sns.lineplot(x='error_rate', y='dfc_rate_No_retry', data=merged, ax=ax1,  label='Decoder failure rate with no retry')
    sns.lineplot(x='error_rate', y=merged['dfc_rate_w_retry']+ epsilon, data=merged, ax=ax1, label='Decoder failure rate with up to 4 retry')
    sns.lineplot(x='error_rate', y='lfc_total_rate', data=merged, ax=ax1,  label='Logical failure rates' , alpha=0.6)
    ax1.set_ylabel('Failure Rate')
    ax1.set_xlabel('Error Rate')

    # Second Y-axis: other metric
    ax2 = ax1.twinx()
    ax2.set_ylim(0.8, 2)  
    sns.lineplot(x='error_rate', y='avg_decoder_trial_w_retry', data=merged, ax=ax2, color='green', label='Avg number of trials',  linestyle='--')
    ax2.set_ylabel('Avg number of trials')

    # Combine legends from both axes
    lines_1, labels_1 = ax1.get_legend_handles_labels()
    ax1.legend(lines_1, labels_1, loc='upper left')

    plt.title("Failure rates")
    
    plt.legend()
    plt.savefig("report_plots/retry_test.png")
    
def plot_for_exp_red(r1):

    # Plot manually
    plt.figure(figsize=(8, 6))
    plt.yscale('log')
    sns.lineplot(data=r1, x='code_distance', y='failure_rate', marker="o")
    plt.title("Failure rate for different code sizes (at p=7%)")
    plt.xlabel(f"Code distance")
    plt.ylabel("Failure Rate")
    plt.grid(True)
    
    plt.savefig(f"{output_dir}/exp_red.png")
    plt.close()
    
def gen_plot(df_results, run_spec={}, output_dir="outputs"):
    # Assumes df_results is already loaded
    df_interp = pd.DataFrame()
    df_points = pd.DataFrame()

    # Filter dataset
    df_results_2 = df_results[
        (df_results['code_distance'] > 20) &
        (df_results['code_distance'] !=70) &
        (df_results['error_rate'] >= 0.08) &
        (df_results['error_rate'] <= 0.09)
    ]

    # 1. Define scaling function
    def scaling_variable(p, d, p_th, nu):
        return (p - p_th) * d**(1 / nu)

    # 2. Universal scaling law model
    def universal_scaling_law(data, a, b, c, p_th, nu):
        p, d = data
        x_scaled = scaling_variable(p, d, p_th, nu)
        return a * x_scaled**2 + b * x_scaled + c

    # 3. Prepare data
    p = df_results_2['error_rate'].values
    f = df_results_2['failure_rate'].values
    d = df_results_2['code_distance'].values

    # 4. Fit model to data
    initial_guess = [1.0, 0.0, 0.1, 0.05, 1.0]
    bounds = ([-np.inf, -np.inf, 0, 0.001, 0.1], [np.inf, np.inf, 1, 0.5, 10])
    # Plotting
    plt.figure(figsize=(8, 6))
    try:
        popt, pcov = curve_fit(
            universal_scaling_law,
            (p, d),
            f,
            p0=initial_guess,
            bounds=bounds
        )
        a, b, c, p_th, nu = popt
        print(f"Fitted threshold p_th = {p_th:.6f}, critical exponent nu = {nu:.3f}")

        # After computing y_smooth for each code_distance group
        for code_distance, group in df_results_2.groupby('code_distance'):
            x = group['error_rate'].values
            y = group['failure_rate'].values

            # Store original points
            group['code_distance'] = code_distance
            df_points = pd.concat(
                [df_points, group[['error_rate', 'failure_rate', 'code_distance']]],
                ignore_index=True
            )

            # Compute interpolated curve using the universal scaling law
            x_smooth = np.linspace(x.min(), x.max(), 100)
            x_scaled = scaling_variable(x_smooth, code_distance, p_th, nu)
            y_smooth = a * x_scaled**2 + b * x_scaled + c

            temp_df = pd.DataFrame({
                'error_rate': x_smooth,
                'failure_rate': y_smooth,
                'code_distance': code_distance
            })
            df_interp = pd.concat([df_interp, temp_df], ignore_index=True)

            # === Plot residual lines ===
            for xi, yi in zip(x, y):
                x_scaled_i = scaling_variable(np.array([xi]), code_distance, p_th, nu)
                y_model = a * x_scaled_i**2 + b * x_scaled_i + c
                plt.plot([xi, xi], [yi, y_model[0]], color='gray', linestyle='--', linewidth=0.8)

    except RuntimeError as e:
        print("Scaling fit failed:", e)

    
    sns.set_palette("tab10", n_colors=8)
    sns.scatterplot(data=df_points, x='error_rate', y='failure_rate', hue='code_distance', marker='o', edgecolor='black', s=20, legend=False)
    sns.lineplot(data=df_interp, x='error_rate', y='failure_rate', hue='code_distance', legend=True)

    plt.xlabel("Qubits Error Rate")
    plt.ylabel("Failure Rate")
    plt.grid(True)
    plt.legend(title="Code Distance")

    plt.savefig(f"{output_dir}/fail_rate_vs_error_rate_by_code_distance.png")
    plt.close()
    
    failure_avg = df_results.groupby('error_rate')[['dfc_rate', 'lfcx_rate', 'lfcz_rate']].mean().reset_index()
    failure_avg['lfc_total_rate'] = failure_avg['lfcx_rate'] + failure_avg['lfcz_rate']
    

    plt.figure(figsize=(8, 6))
    sns.lineplot(data=failure_avg, x='error_rate', y='dfc_rate', label='Decoder Failure Rate', marker="o")
    sns.lineplot(data=failure_avg, x='error_rate', y='lfc_total_rate', label='Logic Failure Rate', marker="o")
    plt.title("Average Failure Rates vs Error Rate")
    plt.xlabel("Error Rate")
    plt.ylabel("Failure Rate")
    plt.grid(True)
    plt.legend()
    plt.subplots_adjust(bottom=0.2)
    # Add explanatory text clearly below the plot
    plt.figtext(0.5, 0.02, 
                f"This plot shows the relationship between error rate and failure rate for different code sizes. There is 2 types of failure that may occure, either the decoder fail to eliminate all anyons (decoder failure) or the correction results in applying logic gate to the logical state (Logical failure).",
                wrap=True, horizontalalignment='center', fontsize=10)
    
    plt.savefig(f"{output_dir}/avg_failure_types_vs_error_rate.png")
    plt.close()


    plt.figure(figsize=(8, 6))

    # Add total convergence time column if not already done
    df_results['total_convergence_time'] = np.maximum   (df_results['avg_convergence_time_x'], df_results['avg_convergence_time_z'])

    sns.lineplot(
        data=df_results,
        x='error_rate',
        y='total_convergence_time',
        hue='code_distance',
        marker="o"
    )

    plt.title("Total Convergence Time vs Error Rate")
    plt.xlabel("Error Rate")
    plt.ylabel("Convergence Time")
    plt.grid(True)
    plt.legend(title="Code Size")

    plt.savefig(f"{output_dir}/total_convergence_time.png")
    plt.close()



    plt.figure(figsize=(8, 6))
    sns.lineplot(data=df_results, x='error_rate', y='avg_decoding_time', hue='code_distance', marker="o")
    plt.title("Average decoding time vs Data Qubit Error Rate")
    plt.xlabel("Qubits Error Rate")
    plt.ylabel("Average decoding time")
    plt.grid(True)
    plt.legend(title="Code Size")
    plt.subplots_adjust(bottom=0.2)
    # Add explanatory text clearly below the plot
    plt.figtext(0.5, 0.02, 
                f"_",
                wrap=True, horizontalalignment='center', fontsize=10)
    plt.savefig(f"{output_dir}/avg_decoding_time.png")
    plt.close()

    plt.figure(figsize=(8, 6))
    sns.lineplot(data=df_results, x='error_rate', y='avg_check_time', hue='code_distance', marker="o")
    plt.title("Average verifier execution time vs Data Qubit Error Rate")
    plt.xlabel("Qubits Error Rate")
    plt.ylabel("Average decoding time (i.e simulation of the cellular automaton)")
    plt.grid(True)
    plt.legend(title="Code size")
    plt.subplots_adjust(bottom=0.2)
    # Add explanatory text clearly below the plot
    plt.figtext(0.5, 0.02, 
                f"Check that verifying solution isn't a bottleneck (it should be linear on the number of anyons moves), it should be fast compared to decoding time",
                wrap=True, horizontalalignment='center', fontsize=10)
    plt.savefig(f"{output_dir}/avg_check_time.png")
    plt.close()
    
    


def plot_fig_1():
    dataframes = {
        '10': pd.read_pickle("test_outputs/replicas_test/80db8482mirror2"+"/test_data"),
        '30': pd.read_pickle("test_outputs/replicas_test/84cfb605mirror2"+"/test_data"),
        '50': pd.read_pickle("test_outputs/replicas_test/cb8eabb8mirror2"+"/test_data")
        # Add more sizes as needed
    }
    
    print(dataframes['10'])
    
    diff_dfs = []

    for size_label, df in dataframes.items():
        # Pivot to get failure_rate for mirror_size 3 and 10 side-by-side
        pivoted = df.pivot(index='error_rate', columns='mirror_size', values='failure_rate')
        
        # Ensure mirror_size 3 and 10 exist
        if 3 in pivoted.columns and 10 in pivoted.columns:
            pivoted['diff'] =  pivoted[3] - pivoted[10]
            pivoted = pivoted.reset_index()
            pivoted['size'] = size_label
            diff_dfs.append(pivoted[['error_rate', 'diff', 'size']])

    # Combine all into one DataFrame
    plot_df = pd.concat(diff_dfs, ignore_index=True)

    plt.figure(figsize=(10, 6))

        # Get the unique sizes and corresponding colors from the Seaborn palette
    sizes = sorted(plot_df['size'].unique())  # Ensure consistent order
    palette = sns.color_palette(n_colors=len(sizes))
    color_map = dict(zip(sizes, palette))

    # Plot the raw data with consistent color mapping
    sns.lineplot(
        data=plot_df,
        x='error_rate',
        y='diff',
        hue='size',
        palette=color_map,
        marker='o'
    )

    # Fit and plot least squares lines using the same colors
    x_vals = np.linspace(plot_df['error_rate'].min(), plot_df['error_rate'].max(), 100)

    for s in sizes:
        group = plot_df[plot_df['size'] == s]
        x = group['error_rate'].values
        y = group['diff'].values

        # Fit a linear model (least squares)
        coeffs = np.polyfit(x, y, deg=1)
        y_fit = np.polyval(coeffs, x_vals)

        # Use same color as original plot
        plt.plot(x_vals, y_fit, linestyle='--', color=color_map[s])

    # Final plot tweaks
    plt.title('Failure rate reduction between m=3 and m=10 with Least Squares Lines')
    plt.ylabel('Failure Rate Difference')
    plt.xlabel("Error Rate")
    plt.grid(True)
    plt.tight_layout()
    plt.legend(title='Size')
    plt.savefig("report_plots/m_diff.png")
    plt.show()
    
    plt.close()
